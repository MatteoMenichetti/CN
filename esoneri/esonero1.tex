\section{Esonero 1}
\subsection{A.A. 2022/23}
\begin{enumerate}
    \item Come si definisce la precisione di macchina di un'aritmetica finita? Quando vale la precisione macchina della doppia precisione IEEE?
    \item Cosa è il fenomeno della cancellazione numerica?
    \item Derivare il metodo di Newton per la ricerca della radice di una funzione e dimostrare che esso converge quadraticamente a radici semplici.
    \item Derivare il metodo di accelerazione di Aitken.
    \item Scrivere in modo professionale una \textit{function} Matlab che risolva efficientemente un sistema triangolare superiore.
    \item Sotto quali condizioni esiste la fattorizzazione $LU$ di una matrice nonsingolare? Definire cosa si intente per matrice a \textit{diagonale dominante}. Dimostrare che una matrice diagonale dominante è fattorizzabile $LU$.
    \item Definire cosa è il numero di condizionamento di una matrice. Spiegarne il significato.
    \item Definire la soluzione nel senso dei minimi quadrati di un sistema lineare sovra-dimensionato a rango pieno. Dimostrare l'esistenza ed unicità.
\end{enumerate}

\paragraph{1.} Se un'aritmetica finita in base $b$ utilizza $m$ cifre per la mantissa di un numero di macchina normalizzato, allora la precisione di macchina $u$ è definita come
\begin{equation*}
    u=
    \begin{cases}
        b^{1-m},& \text{con troncamento alla $m-$esima cifra;}\\
        \frac{1}{2}b^{1-m}, & \text{con arrotondamento alla $m-$esima cifra.}
    \end{cases}
\end{equation*}
Essa fornisce una maggiorazione uniforme per l'errore di rappresentazione, per i numeri di macchina normalizzati.

\noindent La doppia precisione IEEE utilizza:
\begin{itemize}
    \item base $b=2$;
    \item $m=53$ cifre;
    \item arrotondamento.
\end{itemize}
Pertanto, $u=\frac{1}{2}2^{1-53}=2^{-53}\approx 1\cdot 10^{-16}$.

\paragraph{2.}
La cancellazione numerica è dovuta al malcondizionamento della somma algebrica, nel caso i due numeri da sommare siano quasi opposti. In questo caso il numero di condizione della somma $x+y$ vale
\begin{equation*}
    \kappa=\frac{|x|+|y|}{|x+y|},
\end{equation*}
che non è limitato se $x\approx -y$.

\paragraph{3.} Il metodo di Newton è un metodo iterativo per trovare la radice di $f$, con $f:\mathbb R\rightarrow\mathbb R$, basata su una linearizzazione locale della funzione $f(x)$ nell'approssimazione corrente $x_n$: data la retta tangente il grafico di $f(x)$ nel punto $(x_n, f(x_n))$, 
\begin{equation*}
    r\;:\;y-f(x_n)=f'(x_n)(x-x_n),
\end{equation*}
$x_{n+1}$ è ricavata come l'ascissa per cui
\begin{equation*}
    y=0\;:\;x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)},\, n=0,1,\hdots.
\end{equation*}
Il metodo di Newton a radici semplici ha convergenza quadratica (Teorema \ref{th:metodo_newton_converge_quadraticamente}).

\paragraph{4.}
Il metodo di accelerazione di Aitken è utilizzato per ripristinare la convergenza quadratica del metodo di Newton verso radici multiple, per cui la convergenza è lineare. Definito $e_n=x_n-\overline{x}$ l'errore al passo $n$, asintoticamente, $e_n\approx c \, e_{n-1}$ ed $e_{n+1}\approx c\, e_n$, con $c$ costante non nota dell'errore. Dividendo membro a membro è ottenuto
\begin{equation*}
    \frac{e_{n+1}}{e_n}\approx\frac{e_n}{e_{n-1}}.
\end{equation*}
Data  l'uguaglianza, $\overline x_n\approx \overline x$ il valore che la soddisfa, è ottenuta
\begin{equation*}
    \frac{\overline x_n - x_{n+1}}{\overline x_n - x_n} =\frac{\overline x_n - x_n}{\overline x_n - x_{n-1}},
\end{equation*}
dalla quale 
\begin{equation*}
    (\overline x_n-x_{n+1})(\overline x_n - x_{n-1})=(\overline x_n - x_n)^2.
\end{equation*}
Quindi
\begin{equation*}
    \overline x_n = \frac{x^2_n-x_{n+1}\,x_{n-1}}{2x_n - x_{n+1} - x_{n-1}}.
\end{equation*}
Data questa approssimazione, con due passi di Newton, è ottenuta una nuova approssimazione, $\{\overline x_n\}$, che converge quadraticamente a $\overline x$. Il metodo di accelerazione di Aitkien è preferibile al metodo di Newton modificato quando la molteplicità della radice non è nota.

\paragraph{5.} Vedere Algoritmo \ref{alg:sistTriangSup}.

\begin{algorithm}
\caption{Implementazione esercizio 5.}\label{alg:sistTriangSup}
    \begin{lstlisting}[style=Matlab-editor]
    function x = sollu(U, b)
    %   
    %   function x = sollu(U, b)
    %
    %   Risoluzione del sistema triangolare superiore Ux = b.
    %
    %  Input:
    %   U - matrice dei coefficienti (di cui si utilizza la sola porzione triangolare superiore);
    %
    %  Output:
    %   x - soluzione.
    %
    [m,n] = size(U);
    if m ~= n || n ~= length(b), error, end
    x = b(:);
    for i = n : -1 : 1
        if U(i,i) == 0, error, end
        x(i) = x(i)/U(i,i);
        x(1:i-1) = x(i) * U(1:i-1, i);
    end
    return
    \end{lstlisting}
\end{algorithm}

\paragraph{6.} Data una matrice nonsingolare $A\in\mathbb R^{n\times n}$, la fattorizzazione $LU$ di $A$ esiste se e solo se, data $A_k$ la sottomatrice principale di ordine $k$ di $A$, risulta: $\det(A_k)\neq 0,\; k=1,\hdots, n.$ La matrice $A=(a_{ij})\in\mathbb R^{n\times n}$ è detta diagonale dominante:
\begin{itemize}
    \item per righe, se $\forall i=1,\hdots, n: |a_{ii}|>\sum_{j=1, j\neq i}^n|a_{ij}|$;
    \item per colonne, se $\forall j=1,\hdots, n: |a_{jj}|>\sum_{i=1, i\neq j}^n |a_{ij}|$.
\end{itemize}
Una matrice diagonale dominante è fattorizzabile $LU$. E' sufficiente dimostrare che se $A$ è diagonale dominante allora $A$ è nonsingolare in quando implicherà che $\forall k=1,\hdots, n: A_k$ è diagonale dominante e, quindi, nonsingolare.

Vale la seguente proprietà: $A$ diagonale dominante per righe/colonne s.se $A^T$ diagonale dominante per colonne/righe; quindi è sufficiente dimostrare che $A$ è diagonale dominante per righe per dimostrare che $A$ è nonsingolare. Utile il Teorema \ref{th:matrice_diagonale_dominante_nonsingolare}.

\paragraph{7.} Lo studio del condizionamento di un sistema lineare
\begin{equation*}
    Ax=b
\end{equation*}
consiste nel verificare in che modo la soluzione è perturbata. Sarà studiato
\begin{equation*}
    (A+\Delta A)(x+\Delta x) = b + \Delta b,
\end{equation*}
con $\Delta A$ e $\Delta b$ perturbazioni note.

È ottenuto
\begin{equation*}
    \frac{||\Delta x||}{||x||}\leq ||A||\cdot||A^{-1}||\cdot\left(\frac{||\Delta b||}{||b||} + \frac{||\Delta A||}{||A||}\right),
\end{equation*}
dove, tramite l'utilizzo di una norma su vettore e la corrispondente indotta su matrice,
\begin{equation*}
    \kappa(A)=||A||\cdot ||A^{-1}||
\end{equation*}
è il numero di condizione di $A$.

\paragraph{8.} Dato il sistema lineare sovradimensionato
\begin{equation}\label{eq:sistLinSovradEsonero}
    Ax=b,\quad A\in\mathbb R^{m\times n},\quad m>n=rank(A),\, x\in\mathbb R^n,\, b\in\mathbb R^m,
\end{equation}
è definita soluzione di (\ref{eq:sistLinSovradEsonero}) nel senso dei minimi quadrati il vettore $x$ che minimizza
\begin{equation*}
    ||r||_2^2=||Ax-b||_2^2=\sum_{i=1}^nr_i^2,
\end{equation*}
dove $r_i$ è la $i-$esima componente di $r$.

\noindent Per dimostrare l'esistenza ed unicità di $x$ è necessario il Teorema \ref{th:esistenza_fattorizzazione_QR}. Pertanto, rimane valida (\ref{eq:norma_2_residuo_minimo}) scegliendo $x$ come soluzione di (\ref{eq:Rx=g1}). Poiché $\widehat R$ è nonsingolare, questa soluzione esiste ed è unica.